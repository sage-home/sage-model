#!/usr/bin/env python3
"""
Group Evolution Six-Panel Plotter

Creates a six-panel figure showing the evolution of group properties in different mass bins.
Uses the group_aggregated_properties_evolution.csv file generated by the SAGE evolution tracker.

Usage:
    python plot_group_evolution_panels.py --input group_aggregated_properties_evolution.csv
    python plot_group_evolution_panels.py --input evolution.csv --mass_bins 1e10 1e11 1e12 1e13
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import argparse
from pathlib import Path
import warnings
warnings.filterwarnings('ignore')

try:
    from scipy.integrate import quad
    SCIPY_AVAILABLE = True
except ImportError:
    print("Warning: scipy not available. Using approximate cosmological calculations.")
    SCIPY_AVAILABLE = False

def redshift_to_lookback_time_approx(z, H0=70.0, Om0=0.27, OL0=0.73):
    """
    Approximate conversion from redshift to lookback time.
    Uses fitting function when scipy is not available.
    """
    if z <= 0:
        return 0.0
    
    # Hubble time in Gyr
    t_H = 9.78 / (H0 / 100.0)  # Approximate Hubble time
    
    # Approximate fitting function for ΛCDM cosmology
    # This is reasonably accurate for z < 6
    if z < 0.1:
        # Linear approximation for low z
        return t_H * z * 0.7
    else:
        # More accurate approximation for higher z
        Ez = np.sqrt(Om0 * (1 + z)**3 + OL0)
        # Approximate integral using simple formula
        return t_H * (2.0 / 3.0) * (1.0 / Ez) * (1 - 1.0 / (1 + z)**1.5)

def lookback_time_to_redshift(t_lookback, H0=70.0, Om0=0.27, OL0=0.73, max_z=20):
    """
    Convert lookback time to redshift using cosmological parameters.
    
    Parameters:
    t_lookback: lookback time in Gyr
    H0: Hubble constant in km/s/Mpc
    Om0: matter density parameter
    OL0: dark energy density parameter
    max_z: maximum redshift to search
    
    Returns:
    redshift: float
    """
    if t_lookback <= 0:
        return 0.0
    
    if SCIPY_AVAILABLE:
        # Use precise integration
        c = 299792.458
        t_H = (c / H0) / (3.0857e16 / 3.15576e16)  # Convert to Gyr
        
        def integrand(z_prime):
            """Integrand for lookback time calculation."""
            return 1.0 / ((1 + z_prime) * np.sqrt(Om0 * (1 + z_prime)**3 + OL0))
        
        def lookback_for_z(z):
            """Calculate lookback time for a given redshift."""
            if z <= 0:
                return 0.0
            try:
                integral, _ = quad(integrand, 0, z)
                return t_H * integral
            except:
                return t_H * z / np.sqrt(Om0 * (1 + z)**3 + OL0)
        
        # Binary search to find redshift that gives the target lookback time
        z_low, z_high = 0.0, max_z
        tolerance = 0.01  # Gyr
        
        for _ in range(50):  # Maximum iterations
            z_mid = (z_low + z_high) / 2
            t_mid = lookback_for_z(z_mid)
            
            if abs(t_mid - t_lookback) < tolerance:
                return z_mid
            elif t_mid < t_lookback:
                z_low = z_mid
            else:
                z_high = z_mid
        
        return (z_low + z_high) / 2
    else:
        # Use approximate method - inverse of the approximation
        # Simple iterative solution
        z_guess = t_lookback / 5.0  # Initial guess
        
        for _ in range(20):  # Iterate to converge
            t_calc = redshift_to_lookback_time_approx(z_guess, H0, Om0, OL0)
            error = t_calc - t_lookback
            
            if abs(error) < 0.05:  # 50 Myr tolerance
                return z_guess
            
            # Adjust guess
            if error > 0:
                z_guess *= 0.9
            else:
                z_guess *= 1.1
            
            if z_guess > max_z:
                z_guess = max_z
                break
        
        return z_guess

def create_redshift_axis(ax, time_min, time_max, cosmo_params=None):
    """
    Add a secondary x-axis showing redshift values.
    
    Parameters:
    ax: matplotlib axis object
    time_min, time_max: range of lookback times in Gyr
    cosmo_params: cosmological parameters dictionary
    
    Returns:
    ax2: secondary axis object
    """
    if cosmo_params is None:
        cosmo_params = {'H0': 70.0, 'Omega_m': 0.27, 'Omega_Lambda': 0.73}
    
    # Create secondary x-axis
    ax2 = ax.twiny()
    
    # Define redshift tick positions
    # Choose nice redshift values to display
    if time_max - time_min > 10:  # Long time span
        z_ticks = [0, 0.5, 1, 2, 3, 5]
    elif time_max - time_min > 5:   # Medium time span
        z_ticks = [0, 0.2, 0.5, 1, 1.5, 2]
    else:  # Short time span
        z_ticks = [0, 0.1, 0.2, 0.5, 1]
    
    # Convert redshift ticks to lookback times
    time_ticks = []
    z_labels = []
    
    H0 = cosmo_params['H0']
    Om0 = cosmo_params['Omega_m']
    OL0 = cosmo_params['Omega_Lambda']
    
    for z in z_ticks:
        # Convert redshift to lookback time
        if z == 0:
            t_lookback = 0.0
        else:
            if SCIPY_AVAILABLE:
                # Use precise calculation
                c = 299792.458  # km/s
                t_H = (c / H0) / (3.0857e16 / 3.15576e16)  # Hubble time in Gyr
                
                def integrand(z_prime):
                    return 1.0 / ((1 + z_prime) * np.sqrt(Om0 * (1 + z_prime)**3 + OL0))
                
                try:
                    integral, _ = quad(integrand, 0, z)
                    t_lookback = t_H * integral
                except:
                    t_lookback = redshift_to_lookback_time_approx(z, H0, Om0, OL0)
            else:
                # Use approximation
                t_lookback = redshift_to_lookback_time_approx(z, H0, Om0, OL0)
        
        # Only include if within our time range
        if time_min <= t_lookback <= time_max:
            time_ticks.append(t_lookback)
            z_labels.append(f'{z}')
    
    # Set the ticks and labels
    ax2.set_xlim(ax.get_xlim())  # Match the primary axis limits
    ax2.set_xticks(time_ticks)
    ax2.set_xticklabels(z_labels)
    ax2.set_xlabel('Redshift', fontsize=12)
    
    return ax2

def define_mass_bins(data, mass_column='total_StellarMass', 
                    custom_bins=None, n_bins=3, min_groups_per_bin=5):
    """
    Define mass bins for grouping the data.
    
    Parameters:
    data: DataFrame with group evolution data
    mass_column: Column to use for mass binning
    custom_bins: List of mass bin edges in M_sun (e.g., [1e10, 1e11, 1e12, 1e13])
    n_bins: Number of bins if using automatic binning
    min_groups_per_bin: Minimum number of unique groups per bin
    
    Returns:
    bin_edges: Array of bin edges
    bin_labels: List of bin label strings
    """
    
    # Get the latest snapshot data for each group (for mass assignment)
    latest_data = data.loc[data.groupby('tracking_id')['sequence_number'].idxmax()]
    masses = latest_data[mass_column].dropna()
    
    if len(masses) == 0:
        raise ValueError(f"No valid data found in column {mass_column}")
    
    print(f"Mass binning using {mass_column}")
    print(f"Available mass range: {masses.min():.2e} - {masses.max():.2e} M_sun")
    print(f"Number of groups for binning: {len(masses)}")
    
    if custom_bins is not None:
        # Use custom bins
        bin_edges = np.array(custom_bins)
        print(f"Using custom mass bins: {bin_edges}")
    else:
        # Create automatic bins based on percentiles
        if n_bins == 3:
            # Default: small (bottom 33%), medium (33-67%), large (top 33%)
            percentiles = [0, 33.33, 66.67, 100]
        else:
            percentiles = np.linspace(0, 100, n_bins + 1)
        
        bin_edges = np.percentile(masses, percentiles)
        print(f"Using automatic {n_bins} bins based on mass percentiles")
        print(f"Bin edges: {bin_edges}")
    
    # Create bin labels
    bin_labels = []
    for i in range(len(bin_edges) - 1):
        low = bin_edges[i]
        high = bin_edges[i + 1]
        
        # Format in scientific notation
        if low >= 1e12:
            low_str = f"{low/1e12:.1f}e12"
        elif low >= 1e11:
            low_str = f"{low/1e11:.1f}e11"
        elif low >= 1e10:
            low_str = f"{low/1e10:.1f}e10"
        else:
            low_str = f"{low:.1e}"
            
        if high >= 1e12:
            high_str = f"{high/1e12:.1f}e12"
        elif high >= 1e11:
            high_str = f"{high/1e11:.1f}e11"
        elif high >= 1e10:
            high_str = f"{high/1e10:.1f}e10"
        else:
            high_str = f"{high:.1e}"
        
        if i == 0:
            bin_labels.append(f"Small ({low_str}-{high_str} M☉)")
        elif i == 1:
            bin_labels.append(f"Medium ({low_str}-{high_str} M☉)")
        else:
            bin_labels.append(f"Large ({low_str}-{high_str} M☉)")
    
    # Check that each bin has enough groups
    for i in range(len(bin_edges) - 1):
        mask = (masses >= bin_edges[i]) & (masses < bin_edges[i + 1])
        if i == len(bin_edges) - 2:  # Last bin includes upper edge
            mask = (masses >= bin_edges[i]) & (masses <= bin_edges[i + 1])
        n_groups_in_bin = mask.sum()
        
        if n_groups_in_bin < min_groups_per_bin:
            print(f"Warning: Bin {i+1} has only {n_groups_in_bin} groups (minimum {min_groups_per_bin} recommended)")
    
    return bin_edges, bin_labels

def assign_mass_bins(data, bin_edges, mass_column='total_StellarMass'):
    """
    Assign each tracking_id to a mass bin based on its final mass.
    
    Parameters:
    data: DataFrame with evolution data
    bin_edges: Array of mass bin edges
    mass_column: Column to use for mass binning
    
    Returns:
    data_with_bins: DataFrame with 'mass_bin' column added
    """
    
    # Get final mass for each group
    latest_data = data.loc[data.groupby('tracking_id')['sequence_number'].idxmax()]
    
    # Create mapping from tracking_id to mass bin
    tracking_id_to_bin = {}
    
    for _, row in latest_data.iterrows():
        mass = row[mass_column]
        tracking_id = row['tracking_id']
        
        if pd.isna(mass):
            tracking_id_to_bin[tracking_id] = -1  # No bin
            continue
        
        # Find which bin this mass belongs to
        bin_idx = np.digitize(mass, bin_edges) - 1
        
        # Handle edge cases
        if bin_idx < 0:
            bin_idx = 0
        elif bin_idx >= len(bin_edges) - 1:
            bin_idx = len(bin_edges) - 2
        
        tracking_id_to_bin[tracking_id] = bin_idx
    
    # Add mass bin column to original data
    data_with_bins = data.copy()
    data_with_bins['mass_bin'] = data_with_bins['tracking_id'].map(tracking_id_to_bin)
    
    # Print bin statistics
    for bin_idx in range(len(bin_edges) - 1):
        bin_data = data_with_bins[data_with_bins['mass_bin'] == bin_idx]
        n_groups = bin_data['tracking_id'].nunique()
        n_points = len(bin_data)
        print(f"Mass bin {bin_idx}: {n_groups} groups, {n_points} data points")
    
    return data_with_bins

def calculate_median_and_scatter(data, property_col, time_col='lookback_time_gyr'):
    """
    Calculate median and 1-sigma scatter for a property as a function of time.
    
    Parameters:
    data: DataFrame with the data
    property_col: Column name for the property to analyze
    time_col: Column name for time
    
    Returns:
    time_points: Array of time points
    medians: Array of median values
    lower_1sig: Array of 16th percentile values
    upper_1sig: Array of 84th percentile values
    """
    
    # Get unique time points
    time_points = np.sort(data[time_col].unique())
    
    medians = []
    lower_1sig = []
    upper_1sig = []
    
    for time_point in time_points:
        time_data = data[data[time_col] == time_point][property_col].dropna()
        
        if len(time_data) == 0:
            medians.append(np.nan)
            lower_1sig.append(np.nan)
            upper_1sig.append(np.nan)
            continue
        
        # Calculate percentiles (16th, 50th, 84th for 1-sigma equivalent)
        if len(time_data) >= 3:
            percentiles = np.percentile(time_data, [16, 50, 84])
            lower_1sig.append(percentiles[0])
            medians.append(percentiles[1])
            upper_1sig.append(percentiles[2])
        else:
            # Not enough data for reliable percentiles
            median_val = np.median(time_data)
            medians.append(median_val)
            lower_1sig.append(median_val)
            upper_1sig.append(median_val)
    
    return np.array(time_points), np.array(medians), np.array(lower_1sig), np.array(upper_1sig)

def plot_group_evolution_panels(data_file, output_dir='./', mass_bins=None, 
                               mass_column='total_StellarMass', figsize=(18, 12)):
    """
    Create six-panel evolution plot.
    
    Parameters:
    data_file: Path to group_aggregated_properties_evolution.csv
    output_dir: Output directory for plots
    mass_bins: Custom mass bin edges (M_sun units)
    mass_column: Column to use for mass binning
    figsize: Figure size
    """
    
    print("Loading group evolution data...")
    data = pd.read_csv(data_file)
    print(f"Loaded {len(data)} data points for {data['tracking_id'].nunique()} groups")
    
    # Define mass bins
    bin_edges, bin_labels = define_mass_bins(data, mass_column, mass_bins)
    
    # Assign mass bins
    data_binned = assign_mass_bins(data, bin_edges, mass_column)
    
    # Define properties to plot
    properties = [
        {'name': 'Halo Mass', 'column': 'total_Mvir', 'unit': 'M☉', 'log': True, 'group_property': True},
        {'name': 'Stellar Mass', 'column': 'total_StellarMass', 'unit': 'M☉', 'log': True, 'group_property': False},
        {'name': 'HI Gas', 'column': 'total_HI_gas', 'unit': 'M☉', 'log': True, 'group_property': False},
        {'name': 'H2 Gas', 'column': 'total_H2_gas', 'unit': 'M☉', 'log': True, 'group_property': False},
        {'name': 'Black Hole Mass', 'column': 'total_BlackHoleMass', 'unit': 'M☉', 'log': True, 'group_property': False},
        {'name': 'Star Formation Rate', 'column': 'total_SFR', 'unit': 'M☉ yr⁻¹', 'log': True, 'group_property': False}
    ]
    
    # Handle missing SFR column - create it from disk + bulge components
    if 'total_SFR' not in data_binned.columns:
        if 'total_SfrDisk' in data_binned.columns and 'total_SfrBulge' in data_binned.columns:
            data_binned['total_SFR'] = data_binned['total_SfrDisk'].fillna(0) + data_binned['total_SfrBulge'].fillna(0)
            print("Created total_SFR from total_SfrDisk + total_SfrBulge")
        else:
            print("Warning: No SFR data available")
    
    # Handle halo mass - if not in aggregated data, use group catalog values
    if 'total_Mvir' not in data_binned.columns:
        print("Warning: total_Mvir not found in evolution data. Using group center coordinates for halo mass placeholder.")
        # Create placeholder - in real analysis you'd want to merge with group catalog
        data_binned['total_Mvir'] = np.nan
    
    # Set up the figure
    fig, axes = plt.subplots(2, 3, figsize=figsize)
    axes = axes.flatten()
    
    # Get plasma colors for mass bins
    n_bins = len(bin_labels)
    colors = plt.cm.plasma(np.linspace(0.2, 0.8, n_bins))  # Avoid extremes for better visibility
    
    # Get time range for x-axis
    time_range = data_binned['lookback_time_gyr'].dropna()
    time_min, time_max = time_range.min(), time_range.max()
    
    # Plot each property
    for prop_idx, prop in enumerate(properties):
        ax = axes[prop_idx]
        
        # Check if property exists in data
        if prop['column'] not in data_binned.columns:
            ax.text(0.5, 0.5, f"No data for {prop['name']}", 
                   transform=ax.transAxes, ha='center', va='center', fontsize=14)
            ax.set_title(prop['name'])
            continue
        
        # Plot each mass bin
        for bin_idx in range(n_bins):
            bin_data = data_binned[data_binned['mass_bin'] == bin_idx]
            
            if len(bin_data) == 0:
                continue
            
            # Calculate median and scatter
            times, medians, lower_1sig, upper_1sig = calculate_median_and_scatter(
                bin_data, prop['column'])
            
            # Remove NaN values for plotting
            valid_mask = ~(np.isnan(medians) | np.isnan(lower_1sig) | np.isnan(upper_1sig))
            if not valid_mask.any():
                continue
                
            times_valid = times[valid_mask]
            medians_valid = medians[valid_mask]
            lower_1sig_valid = lower_1sig[valid_mask]
            upper_1sig_valid = upper_1sig[valid_mask]
            
            # Plot median line
            ax.plot(times_valid, medians_valid, color=colors[bin_idx], 
                   linewidth=2.5, label=bin_labels[bin_idx], alpha=0.9)
            
            # Plot 1-sigma shading
            ax.fill_between(times_valid, lower_1sig_valid, upper_1sig_valid, 
                           color=colors[bin_idx], alpha=0.3)
        
        # Set scale and labels
        if prop['log']:
            ax.set_yscale('log')
        
        ax.set_xlabel('Lookback Time [Gyr]')
        ax.set_ylabel(f"{prop['name']} [{prop['unit']}]")
        ax.set_title(prop['name'], fontsize=14, fontweight='bold')
        # ax.grid(True, alpha=0.3)
        
        # Set consistent x-axis limits
        ax.set_xlim(time_min - 0.2, time_max + 0.2)
        
        # Add redshift axis at the top
        cosmo_params = {'H0': 70.0, 'Omega_m': 0.27, 'Omega_Lambda': 0.73}
        ax2 = create_redshift_axis(ax, time_min - 0.2, time_max + 0.2, cosmo_params)
        
        # Add legend only to first panel
        if prop_idx == 0:
            ax.legend(loc='best', fontsize=10)
    
    # Add overall title
    redshift_min = data_binned['redshift'].min()
    redshift_max = data_binned['redshift'].max()
    fig.suptitle(f'Group Property Evolution by Mass '
                f'z = {redshift_min:.2f} → {redshift_max:.2f} '
                f'({time_min:.1f} → {time_max:.1f} Gyr lookback)', 
                fontsize=16, fontweight='bold')
    
    plt.tight_layout()
    plt.subplots_adjust(top=0.90)
    
    # Save plot
    output_path = Path(output_dir)
    output_path.mkdir(exist_ok=True)
    
    output_file = output_path / 'group_evolution_six_panels.png'
    plt.savefig(output_file, dpi=300, bbox_inches='tight')
    print(f"Saved six-panel evolution plot to {output_file}")
    
    # Also save as PDF for publications
    output_file_pdf = output_path / 'group_evolution_six_panels.pdf'
    plt.savefig(output_file_pdf, bbox_inches='tight')
    print(f"Saved PDF version to {output_file_pdf}")
    
    # plt.show()
    
    # Print summary statistics
    print(f" Evolution Plot Summary:")
    print(f"Mass bins used: {len(bin_labels)}")
    print(f"Time range: {time_min:.1f} - {time_max:.1f} Gyr lookback")
    print(f"Redshift range: z = {redshift_min:.2f} - {redshift_max:.2f}")
    print(f"Properties plotted: {[p['name'] for p in properties]}")
    for i, label in enumerate(bin_labels):
        bin_data = data_binned[data_binned['mass_bin'] == i]
        n_groups = bin_data['tracking_id'].nunique() if len(bin_data) > 0 else 0
        print(f"  {label.replace(' ', ' ')}: {n_groups} groups")

def main():
    parser = argparse.ArgumentParser(description='Plot group evolution in six panels')
    
    parser.add_argument('--input', type=str, required=True,
                       help='Path to group_aggregated_properties_evolution.csv')
    parser.add_argument('--output_dir', type=str, default='./',
                       help='Output directory for plots')
    parser.add_argument('--mass_bins', type=float, nargs='+', default=None,
                       help='Custom mass bin edges in M_sun (e.g., --mass_bins 1e10 1e11 1e12 1e13)')
    parser.add_argument('--mass_column', type=str, default='total_StellarMass',
                       help='Column to use for mass binning')
    parser.add_argument('--figsize', type=float, nargs=2, default=[18, 12],
                       help='Figure size in inches (width height)')
    
    args = parser.parse_args()
    
    # Check if input file exists
    if not Path(args.input).exists():
        print(f"Error: Input file {args.input} not found!")
        print("Make sure you've run the group evolution analysis first.")
        return
    
    print("="*60)
    print("GROUP EVOLUTION SIX-PANEL PLOTTER")
    print("="*60)
    print(f"Input file: {args.input}")
    print(f"Output directory: {args.output_dir}")
    
    if args.mass_bins:
        print(f"Custom mass bins: {args.mass_bins} M_sun")
    else:
        print("Using automatic mass bins (33rd/67th percentiles)")
    
    # Create the plot
    plot_group_evolution_panels(
        args.input, 
        args.output_dir, 
        args.mass_bins,
        args.mass_column,
        tuple(args.figsize)
    )
    
    print(" " + "="*60)
    print("PLOTTING COMPLETE")
    print("="*60)

if __name__ == "__main__":
    main()